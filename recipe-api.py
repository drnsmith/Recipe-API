# -*- coding: utf-8 -*-
"""foodtech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TVhmB4grAxyJ5Qe8sXuYLkYVh7U35yoi

Load CSV File in Google Colab
"""



# Install necessary libraries (if not already installed)
!pip install pandas numpy openai transformers

import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# List files in your Google Drive directory
os.listdir('/content/drive/My Drive')

# Import required libraries
import pandas as pd

# Define the file path (adjust if the file is in a subdirectory)
file_path = '/content/drive/My Drive/parsed_clustered_recipe_with_difficulty.csv'

# Load the CSV file into a pandas DataFrame
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
print("Dataset loaded successfully!")
df.head()

# Check dataset info
df.info()

# Check for missing values
print("Missing values in each column:")
print(df.isnull().sum())

# Drop rows with missing values (if these columns are critical)
df_cleaned = df.dropna(subset=['preprocessed_ingredients', 'preprocessed_directions', 'preprocessed_full_recipe'])

# Alternatively, fill missing values with an empty string
df_filled = df.fillna('')

# Confirm no missing values remain
print("Missing values after cleaning:")
print(df_filled.isnull().sum())

#!pip install sentence-transformers

# Display all column names
print(df.columns)

import pandas as pd
from google.colab import drive

# Mount Google Drive to access the dataset
drive.mount('/content/drive')

# Load the dataset from Google Drive
file_path = '/content/drive/My Drive/parsed_clustered_recipe_with_difficulty.csv'
df_cleaned = pd.read_csv(file_path)

# Fill missing values in the target column
df_cleaned['preprocessed_full_recipe'] = df_cleaned['preprocessed_full_recipe'].fillna('')

# Verify the dataset
print("Dataset loaded successfully. First few rows:")
print(df_cleaned[['title', 'preprocessed_full_recipe']].head())

from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from google.colab import drive

# Mount Google Drive to access the dataset
drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/My Drive/parsed_clustered_recipe_with_difficulty.csv'
df_cleaned = pd.read_csv(file_path)

# Fill missing values in the target column
df_cleaned['preprocessed_full_recipe'] = df_cleaned['preprocessed_full_recipe'].fillna('')

# Load the SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Extract the text column for embeddings
sentences = df_cleaned['preprocessed_full_recipe'].tolist()

# Generate embeddings
print("Generating embeddings...")
embeddings = model.encode(sentences, batch_size=32, show_progress_bar=True)

# Add embeddings as a new column
df_cleaned['generated_embedding'] = embeddings.tolist()
print("Embeddings generated and added to the DataFrame.")

# Save the updated dataset to Google Drive
csv_path = '/content/drive/My Drive/updated_recipes_with_generated_embeddings.csv'
df_cleaned.to_csv(csv_path, index=False)
print(f"Updated dataset saved to: {csv_path}")

# Save only the embeddings as a .npy file
npy_path = '/content/drive/My Drive/recipe_embeddings.npy'
np.save(npy_path, embeddings)
print(f"Embeddings saved to: {npy_path}")

import pandas as pd
import numpy as np

# Load the updated dataset
csv_path = '/content/drive/My Drive/updated_recipes_with_generated_embeddings.csv'
df_cleaned = pd.read_csv(csv_path)

# Load the standalone embeddings
npy_path = '/content/drive/My Drive/recipe_embeddings.npy'
embeddings = np.load(npy_path)

# Verify the dataset
print("Dataset sample:")
print(df_cleaned.head())

# Verify the embeddings
print(f"Embeddings shape: {embeddings.shape}")
print(f"Sample embedding (first recipe): {embeddings[0]}")

import pandas as pd
import numpy as np

from google.colab import drive

# Mount Google Drive to access the dataset
drive.mount('/content/drive')
# Reload the dataset
csv_path = '/content/drive/My Drive/updated_recipes_with_generated_embeddings.csv'
df_cleaned = pd.read_csv(csv_path)

# Reload the embeddings
npy_path = '/content/drive/My Drive/recipe_embeddings.npy'
embeddings = np.load(npy_path)

# Verify reload
print(f"Dataset rows: {len(df_cleaned)}, Embedding shape: {embeddings.shape}")
print(f"Sample embedding: {embeddings[0]}")









"""Build a Simple Recommendation System: Use filtering or embeddings for ingredient-based recipe recommendations."""

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Load the same model used for recipe embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

def get_query_embedding(ingredients):
    """
    Generate an embedding for the user input ingredients.
    """
    query_embedding = model.encode(ingredients, convert_to_numpy=True)
    return query_embedding

"""Calculate Similarity and Recommend Recipes"""



def recommend_recipes(user_ingredients, embeddings, df_cleaned, top_n=5):
    """
    Recommend recipes based on user ingredients.

    Args:
    - user_ingredients: String containing user ingredients (e.g., "chicken, garlic, onion").
    - embeddings: Precomputed embeddings for recipes.
    - df_cleaned: DataFrame containing recipe details.
    - top_n: Number of recommendations to return.

    Returns:
    - DataFrame with recommended recipes and their similarity scores.
    """
    # Generate query embedding for user ingredients
    query_embedding = get_query_embedding(user_ingredients)

    # Calculate cosine similarity between query and all recipe embeddings
    similarities = cosine_similarity([query_embedding], embeddings)[0]

    # Get indices of top N most similar recipes
    top_indices = similarities.argsort()[-top_n:][::-1]  # Sort and reverse

    # Extract recommended recipes
    recommended_recipes = df_cleaned.iloc[top_indices].copy()
    recommended_recipes['similarity_score'] = similarities[top_indices]

    return recommended_recipes[['title', 'ingredients', 'directions', 'similarity_score']]

"""Test the Recommendation System
Provide a sample input (e.g., user-selected ingredients) and test the function.
"""

# Example: User input ingredients
user_ingredients = "chicken, garlic, onion"

# Get recommendations
recommended = recommend_recipes(user_ingredients, embeddings, df_cleaned, top_n=5)

# Display recommendations
print("Recommended Recipes:")
print(recommended)

"""Explanation of the Output
Recommended Recipes:

Titles like "Jolly Greens" and "Monday to Friday Turkey Gumbo" are returned as top matches.
These recipes are semantically similar to the input ingredients ("chicken, garlic, onion").
Ingredients:

Each recipe’s ingredient list is displayed, which aligns with the user’s query in terms of semantic similarity.
Directions:

The cooking instructions provide a complete recipe workflow for the user.
Similarity Scores:

Scores (e.g., 0.747949) indicate how closely each recipe matches the user’s input.
Higher scores = better match.

What Works Well
Semantic Matching: The system uses embeddings to find semantically similar recipes rather than relying on exact ingredient matches. For example, recipes with "chicken" and "garlic" but different preparation styles are included.
Flexibility: The system adapts to user inputs with diverse wording or ingredient lists.

"""









"""Integrate an LLM for Recipe Suggestions and Customisation
Use OpenAI’s GPT for dynamic responses.

Overview
Goal: Add a natural language interface to your recipe recommendation system, allowing users to:
 - Get recipe suggestions.
Customise recipes (e.g., "Make it vegan" or "Add garlic").
 - Implementation:
Use an LLM to process user queries and interact with the recommendation system or dataset.
Free Option: Use Hugging Face’s transformers library with a free model like tiiuae/falcon-7b-instruct (instruction-tuned for Q&A).
"""

#!pip install transformers

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the Falcon-7B-Instruct model from Hugging Face
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

print("Model and tokenizer loaded successfully!")

"""Define a Function to Generate Responses
This function will take a user query (e.g., "Make the recipe vegan") and use the LLM to generate a response.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the Falcon-7B-Instruct model from Hugging Face
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token
# or tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # alternative

print("Model and tokenizer loaded successfully!")

def get_recipe_customisation(user_query, context, max_new_tokens=100):
    """
    Generate a recipe suggestion or customisation based on user input.

    Args:
    - user_query: User's customisation request or question.
    - context: Contextual information (e.g., recipe details).
    - max_new_tokens: Number of new tokens to generate.

    Returns:
    - Generated response from the LLM.
    """
    # Combine the user query with the recipe context
    input_text = f"Context:\n{context}\n\nUser Query:\n{user_query}\n\nResponse:"

    # Tokenize input with padding and truncation
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512  # Adjust max length for tokenized input
    )

    # Generate response
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],  # Add attention mask
        max_new_tokens=max_new_tokens,  # Specify new token limit
        pad_token_id=tokenizer.eos_token_id  # Set pad token to eos token
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response

"""Use the LLM with Recipe Data
Test the integration by customising a recipe from your dataset.
"""

# Example: User selects a recipe
recipe_index = 0  # Index of the selected recipe
recipe_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                 f"Ingredients: {df_cleaned['ingredients'][recipe_index]}\n" \
                 f"Directions: {df_cleaned['directions'][recipe_index]}"

# Example user query
user_query = "Make this recipe vegan and suggest a side dish."

# Get the customised recipe suggestion
customised_response = get_recipe_customisation(user_query, recipe_context, max_new_tokens=200)


print("Customised Recipe Response:")
print(customised_response)

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the Falcon-7B-Instruct model from Hugging Face
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token

print("Model and tokenizer loaded successfully!")

# Function to generate recipe customisation
def get_recipe_customisation(user_query, context, max_new_tokens=300):
    """
    Generate a recipe suggestion or customisation based on user input.

    Args:
    - user_query: User's customisation request or question.
    - context: Contextual information (e.g., recipe details).
    - max_new_tokens: Number of new tokens to generate.

    Returns:
    - Generated response from the LLM.
    """
    # Combine the user query with the recipe context
    input_text = f"Context:\n{context}\n\nUser Query:\n{user_query}\n\nResponse:"

    # Tokenize input with padding and truncation
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512  # Adjust max length for tokenized input
    )

    # Generate response with sampling enabled
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],  # Add attention mask
        max_new_tokens=max_new_tokens,  # Specify new token limit
        pad_token_id=tokenizer.eos_token_id,  # Set pad token to eos token
        temperature=0.7,  # Reduce randomness
        top_k=50,  # Focus on top 50 tokens
        top_p=0.9,  # Nucleus sampling
        do_sample=True  # Enable sampling for creative outputs
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Example: User selects a recipe
recipe_index = 0  # Select the first recipe
ingredients = df_cleaned['ingredients'][recipe_index][:100]  # Truncate ingredients
directions = df_cleaned['directions'][recipe_index][:100]  # Truncate directions

recipe_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                 f"Ingredients: {ingredients}\n" \
                 f"Directions: {directions}"

# Example user query
user_query = "Make this recipe vegan and suggest a side dish."

# Get the customised recipe suggestion
customised_response = get_recipe_customisation(user_query, recipe_context, max_new_tokens=300)

print("Customised Recipe Response:")
print(customised_response)

"""What’s Working Well
Creative Adjustments:
The recipe includes thoughtful substitutions and additions, such as avocados for creaminess.
Concise Yet Informative:
Both the veganisation process and side dish suggestion are included.

"""



"""Extend the Functionality
Dynamic Queries:

Allow users to ask varied questions like:
"Reduce the cooking time."
"Add garlic to this recipe."
"Suggest alternative ingredients."

What to Add for Dynamic Queries
Dynamic queries allow users to interact with the system beyond simple recipe suggestions. Below are potential features and the business needs they solve:

1. Multi-step Query Handling
Why? Users might want to ask follow-up questions or refine the recipe further.
"""

# Initialize the session context with the selected recipe
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {df_cleaned['ingredients'][recipe_index][:100]}\n" \
                  f"Directions: {df_cleaned['directions'][recipe_index][:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update the session context with the user's query and model's response
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example: User follows up with another query
user_query = "Suggest a wine pairing for this recipe."
customised_response, session_context = handle_follow_up_query(user_query, session_context)
print("Customised Response:", customised_response)

"""Dietary and Nutritional Filters Why? Customisation based on user preferences or dietary needs is a key business goal in food tech."""

# Initialize the session context with the selected recipe
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {df_cleaned['ingredients'][recipe_index][:100]}\n" \
                  f"Directions: {df_cleaned['directions'][recipe_index][:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update the session context with the user's query and model's response
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example dietary query
dietary_query = "Make this recipe gluten-free."

# Generate response for dietary customisation
customised_dietary_response, session_context = handle_follow_up_query(dietary_query, session_context)
print("Dietary Customisation Response:", customised_dietary_response)





"""Ingredient Substitutions
Why? Allow users to modify recipes dynamically based on available ingredients or preferences.
"""

# Initialize the session context with the selected recipe
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {df_cleaned['ingredients'][recipe_index][:100]}\n" \
                  f"Directions: {df_cleaned['directions'][recipe_index][:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update the session context with the user's query and model's response
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example dietary query
dietary_query = "Make this recipe gluten-free."

ingredient_query = "Replace olive oil with coconut oil."

# Generate response for ingredient substitution
ingredient_substitution_response, session_context = handle_follow_up_query(ingredient_query, session_context)
print("Ingredient Substitution Response:", ingredient_substitution_response)





"""Multi-recipe Comparisons
Why? Let users compare two recipes and choose the one that fits their needs better.


"""

# Initialize the session context with the selected recipe
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {df_cleaned['ingredients'][recipe_index][:100]}\n" \
                  f"Directions: {df_cleaned['directions'][recipe_index][:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update the session context with the user's query and model's response
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example: Compare two recipes
recipe_index_1 = 0
recipe_index_2 = 1

recipe_context_1 = f"Title: {df_cleaned['title'][recipe_index_1]}\nIngredients: {df_cleaned['ingredients'][recipe_index_1]}"
recipe_context_2 = f"Title: {df_cleaned['title'][recipe_index_2]}\nIngredients: {df_cleaned['ingredients'][recipe_index_2]}"

comparison_query = "Which recipe is quicker to prepare?"
comparison_context = f"Recipe 1:\n{recipe_context_1}\n\nRecipe 2:\n{recipe_context_2}"
comparison_response, _ = handle_follow_up_query(comparison_query, comparison_context)
print("Comparison Response:", comparison_response)





"""Dietary Restriction Recommendations
Why? Address diverse user needs by suggesting alternative recipes for specific diets.
"""

# Initialize the session context with the selected recipe
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {df_cleaned['ingredients'][recipe_index][:100]}\n" \
                  f"Directions: {df_cleaned['directions'][recipe_index][:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update the session context with the user's query and model's response
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

dietary_recommendation_query = "Suggest three vegan recipes for dinner."

# Generate recommendations
vegan_recommendations, session_context = handle_follow_up_query(dietary_recommendation_query, session_context)
print("Vegan Recommendations:", vegan_recommendations)

# Initialize the session context with the selected recipe
recipe_index = 0  # Example recipe index
session_context = f"Title: {df_cleaned['title'][recipe_index]}\n" \
                  f"Ingredients: {str(df_cleaned['ingredients'][recipe_index])[:100]}\n" \
                  f"Directions: {str(df_cleaned['directions'][recipe_index])[:100]}"

# Function to handle dynamic queries
def handle_follow_up_query(user_query, session_context, max_new_tokens=200):
    """
    Process a follow-up query dynamically by appending it to the session context.
    """
    # Combine session context and user query
    input_text = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:"

    # Tokenize input with truncation and padding
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    # Generate response with repetition penalty and adjusted sampling
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.8,  # Slightly increase randomness
        top_k=50,  # Focus on top 50 tokens
        top_p=0.9,  # Nucleus sampling
        repetition_penalty=1.2,  # Penalise repetitive tokens
        do_sample=True
    )

    # Decode and update session context
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example query for dietary recommendations
dietary_recommendation_query = "Suggest three vegan recipes for dinner."

# Generate recommendations
vegan_recommendations, session_context = handle_follow_up_query(dietary_recommendation_query, session_context)

# Display the response
print("Vegan Recommendations:")
print(vegan_recommendations)



# Function to filter recipes based on a keyword
def filter_recipes_by_keyword(keyword, dataset, max_results=3):
    """
    Filter recipes in the dataset based on a keyword (e.g., 'vegan').

    Args:
    - keyword: The keyword to filter recipes (e.g., 'vegan').
    - dataset: The dataset containing recipes.
    - max_results: Maximum number of recipes to return.

    Returns:
    - A list of filtered recipes with title, ingredients, and directions.
    """
    filtered_recipes = dataset[dataset['ingredients'].str.contains(keyword, case=False, na=False)]
    return filtered_recipes.head(max_results)

# Function to dynamically respond with recipes
def handle_query_with_recipes(user_query, session_context, keyword, dataset, max_results=3):
    """
    Process a query and dynamically return recipes that match the keyword.

    Args:
    - user_query: User's query string.
    - session_context: Current session context.
    - keyword: Keyword to filter recipes.
    - dataset: Recipe dataset.
    - max_results: Maximum number of recipes to return.

    Returns:
    - Generated response with matching recipes and updated session context.
    """
    # Filter recipes based on keyword
    matching_recipes = filter_recipes_by_keyword(keyword, dataset, max_results)

    # Format the response with recipe details
    if not matching_recipes.empty:
        response = "Here are some recipes:\n"
        for idx, row in matching_recipes.iterrows():
            response += f"Title: {row['title']}\n"
            response += f"Ingredients: {row['ingredients']}\n"
            response += f"Directions: {row['directions']}\n\n"
    else:
        response = f"Sorry, no recipes found for '{keyword}'."

    # Update session context
    session_context += f"\n\nUser Query: {user_query}\nResponse: {response}"
    return response, session_context

# Example: User asks for vegan recipes
dietary_recommendation_query = "Suggest three vegan recipes for dinner."
keyword = "vegan"  # Filter for 'vegan' recipes
max_results = 3

# Generate recommendations
vegan_recommendations, session_context = handle_query_with_recipes(
    dietary_recommendation_query, session_context, keyword, df_cleaned, max_results
)

# Display the response
print("Vegan Recommendations:")
print(vegan_recommendations)

# Function to filter recipes by a single keyword
def filter_recipes_by_keyword(keyword, dataset, max_results=3):
    """
    Filter recipes in the dataset based on a single keyword (e.g., 'vegan').

    Args:
    - keyword: The keyword to filter recipes (e.g., 'vegan').
    - dataset: The dataset containing recipes.
    - max_results: Maximum number of recipes to return.

    Returns:
    - A filtered DataFrame of recipes.
    """
    filtered_recipes = dataset[dataset['ingredients'].str.contains(keyword, case=False, na=False)]
    return filtered_recipes.head(max_results)


# Function to filter recipes by multiple keywords
def filter_recipes_by_multiple_keywords(keywords, dataset, max_results=3):
    """
    Filter recipes in the dataset based on multiple keywords (e.g., 'vegan', 'gluten-free').

    Args:
    - keywords: A list of keywords to filter recipes (e.g., ['vegan', 'gluten-free']).
    - dataset: The dataset containing recipes.
    - max_results: Maximum number of recipes to return.

    Returns:
    - A filtered DataFrame of recipes.
    """
    filtered_recipes = dataset
    for keyword in keywords:
        filtered_recipes = filtered_recipes[filtered_recipes['ingredients'].str.contains(keyword, case=False, na=False)]
    return filtered_recipes.head(max_results)


# Function to dynamically handle dietary recommendation queries
def handle_query_with_recipes(user_query, dataset, max_results=3):
    """
    Process a user query and dynamically return recipes based on keywords.

    Args:
    - user_query: The user query string.
    - dataset: The dataset containing recipes.
    - max_results: Maximum number of recipes to return.

    Returns:
    - A user-friendly string response with recipe recommendations.
    """
    # Example: Extract keywords from user query (for simplicity, we'll hardcode the keywords here)
    if "vegan and gluten-free" in user_query.lower():
        keywords = ["vegan", "gluten-free"]
    elif "vegan" in user_query.lower():
        keywords = ["vegan"]
    elif "gluten-free" in user_query.lower():
        keywords = ["gluten-free"]
    else:
        keywords = []  # No specific keywords detected

    # Filter recipes based on keywords
    if keywords:
        filtered_recipes = filter_recipes_by_multiple_keywords(keywords, dataset, max_results)
    else:
        return "Sorry, I couldn't detect any dietary preferences in your query."

    # Format the response
    if not filtered_recipes.empty:
        response = "Here are some recipes based on your preferences:\n\n"
        for idx, row in filtered_recipes.iterrows():
            response += f"Title: {row['title']}\n"
            response += f"Ingredients: {row['ingredients']}\n"
            response += f"Directions: {row['directions']}\n\n"
    else:
        response = f"Sorry, no recipes found for keywords: {', '.join(keywords)}."

    return response


# Example Dataset and Query
# Assume `df_cleaned` is your pre-loaded recipe dataset
dietary_recommendation_query = "Suggest three vegan and gluten-free recipes for dinner."
max_results = 3

# Generate recommendations dynamically
recommendations = handle_query_with_recipes(dietary_recommendation_query, df_cleaned, max_results)

# Display the final response
print("Response to User Query:")
print(recommendations)

# Example: Categorising recipes based on title or directions
df_cleaned['is_baking'] = df_cleaned['title'].str.contains("cupcake|cake|cookie|bake|brownie", case=False, na=False) | \
                          df_cleaned['directions'].str.contains("bake|oven", case=False, na=False)

# Count baking vs non-baking recipes
baking_count = df_cleaned['is_baking'].sum()
non_baking_count = len(df_cleaned) - baking_count

print(f"Baking recipes: {baking_count}")
print(f"Non-baking recipes: {non_baking_count}")

# Exclude recipes containing baking keywords
non_baking_recipes = df_cleaned[~df_cleaned['directions'].str.contains("bake|oven", case=False, na=False)]
filtered_recipes = non_baking_recipes[
    non_baking_recipes['ingredients'].str.contains("vegan", case=False, na=False) &
    non_baking_recipes['ingredients'].str.contains("gluten-free", case=False, na=False)
]

def generate_non_baking_recipes(keywords, max_results=3):
    prompt = f"Suggest {max_results} vegan and gluten-free recipes that are not baking. Provide title, ingredients, and directions."
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=300,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Example
synthetic_recipes = generate_non_baking_recipes(["vegan", "gluten-free"], max_results=3)
print("Generated Non-Baking Recipes:", synthetic_recipes)





















"""Next Steps
Test the LLM’s outputs for diverse queries and ensure they align with user expectations.
Deploy the system via a web interface (e.g., using Flask or Streamlit).
Explore adding more advanced capabilities like dietary filtering or multilingual support.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the Falcon-7B-Instruct model
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add padding token
tokenizer.pad_token = tokenizer.eos_token

# Function to query the LLM
def query_llm(prompt, max_new_tokens=150):
    """
    Generate a response from the LLM based on the input prompt.

    Args:
    - prompt: The text prompt for the LLM.
    - max_new_tokens: Number of new tokens to generate.

    Returns:
    - The generated response from the LLM.
    """
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Test diverse queries
def test_llm_responses(test_cases, session_context=None):
    """
    Test the LLM with diverse queries.

    Args:
    - test_cases: List of test cases with prompts and expected outcomes.
    - session_context: Optional session context to simulate a dialogue.

    Returns:
    - Dictionary of results for manual review.
    """
    results = {}
    for i, test_case in enumerate(test_cases, start=1):
        user_query = test_case.get("query")
        prompt = f"Context:\n{session_context}\n\nUser Query:\n{user_query}\n\nResponse:" if session_context else user_query
        print(f"Testing Query {i}: {user_query}")
        response = query_llm(prompt)
        results[f"Test Case {i}"] = {
            "query": user_query,
            "response": response,
            "expected_behavior": test_case.get("expected_behavior", "Not Provided")
        }
    return results

# Example diverse test cases
test_cases = [
    {
        "query": "Suggest three recipes for a gluten-free and low-carb dinner.",
        "expected_behavior": "LLM provides three valid recipes with ingredients and cooking methods."
    },
    {
        "query": "How can I make Gazpacho spicier?",
        "expected_behavior": "LLM suggests ways to make Gazpacho spicier, such as adding chili or hot sauce."
    },
    {
        "query": "Recommend a vegan dessert with chocolate.",
        "expected_behavior": "LLM provides a vegan chocolate dessert recipe with clear directions."
    },
    {
        "query": "List quick snacks for a picnic.",
        "expected_behavior": "LLM provides a list of quick snack ideas suitable for a picnic."
    },
    {
        "query": "What wine pairs well with mushroom risotto?",
        "expected_behavior": "LLM suggests appropriate wine pairings like Pinot Noir or Chardonnay."
    }
]

# Optional session context
session_context = "Title: Mushroom Risotto\nIngredients: ['Arborio rice', 'Mushrooms', 'Vegetable stock', 'Parmesan', 'Butter', 'Garlic']\nDirections: ['Sauté garlic and mushrooms in butter. Add rice and stock gradually while stirring. Finish with Parmesan.']"

# Run tests
results = test_llm_responses(test_cases, session_context)

# Display results
for test_case, result in results.items():
    print(f"\n{test_case}:")
    print(f"Query: {result['query']}")
    print(f"Response: {result['response']}")
    print(f"Expected Behavior: {result['expected_behavior']}")

def get_recipe_customisation(user_query, context, max_new_tokens=200):
    """
    Generate a recipe suggestion or customisation based on user input.

    Args:
    - user_query: User's customisation request or question.
    - context: Contextual information (e.g., recipe details).
    - max_new_tokens: Number of new tokens to generate.

    Returns:
    - Generated response from the LLM.
    """
    if not context.strip():
        context = "General cooking context: Please ensure your query is specific for better suggestions."

    # Combine the user query with the recipe context
    input_text = f"Context:\n{context}\n\nUser Query:\n{user_query}\n\nResponse:"

    # Tokenize input
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    # Generate response
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Append the user query and response to the context for continuity
    context += f"\n\nUser Query: {user_query}\nResponse: {response}"

    return response, context

def recommend_recipes(user_ingredients, embeddings, df_cleaned, top_n=5):
    """
    Recommend recipes based on user ingredients.

    Args:
    - user_ingredients: String containing user ingredients (e.g., "chicken, garlic, onion").
    - embeddings: Precomputed embeddings for recipes.
    - df_cleaned: DataFrame containing recipe details.
    - top_n: Number of recommendations to return.

    Returns:
    - DataFrame with recommended recipes and their similarity scores.
    """
    # Generate query embedding for user ingredients
    query_embedding = get_query_embedding(user_ingredients)

    # Calculate cosine similarity between query and all recipe embeddings
    similarities = cosine_similarity([query_embedding], embeddings)[0]

    # Get indices of top N most similar recipes
    top_indices = similarities.argsort()[-top_n:][::-1]  # Sort and reverse

    # Extract recommended recipes
    recommended_recipes = df_cleaned.iloc[top_indices].copy()
    recommended_recipes['similarity_score'] = similarities[top_indices]

    # Add detailed directions and tips to enhance user experience
    for idx in recommended_recipes.index:
        recipe = recommended_recipes.loc[idx]
        recipe_title = recipe['title']
        recipe_directions = recipe['directions']
        recommended_recipes.loc[idx, 'detailed_response'] = (
            f"Title: {recipe_title}\n"
            f"Directions: {recipe_directions}\n"
            f"Serving Tips: This dish can be paired with a light salad or bread for a wholesome meal."
        )

    return recommended_recipes[['title', 'ingredients', 'directions', 'detailed_response', 'similarity_score']]

def recommend_diverse_recipes(user_ingredients, embeddings, df_cleaned, top_n=5, diversity_field='type'):
    """
    Recommend diverse recipes based on user ingredients and a diversity field.

    Args:
    - user_ingredients: User-provided ingredients (e.g., "mushrooms, garlic").
    - embeddings: Precomputed embeddings for recipes.
    - df_cleaned: DataFrame with recipe details.
    - top_n: Number of recommendations to return.
    - diversity_field: Field to ensure diversity (e.g., "type" for baking vs. non-baking).

    Returns:
    - DataFrame with diverse recipe recommendations.
    """
    # Recommend recipes based on embeddings
    recommended_recipes = recommend_recipes(user_ingredients, embeddings, df_cleaned, top_n * 2)

    # Group and sort by diversity field
    grouped = recommended_recipes.groupby(diversity_field).apply(lambda x: x.head(top_n // 2))
    grouped = grouped.sample(frac=1).reset_index(drop=True)

    # Ensure unique and detailed responses
    grouped['detailed_response'] = grouped.apply(
        lambda x: (
            f"Title: {x['title']}\n"
            f"Ingredients: {x['ingredients']}\n"
            f"Directions: {x['directions']}\n"
            f"Tips: For a creative twist, try adding complementary herbs or sauces!"
        ), axis=1
    )

    return grouped.head(top_n)

def test_llm_responses(queries, session_context, max_results=3):
    """
    Test LLM responses for diverse queries with simplified output.

    Args:
    - queries: List of user queries.
    - session_context: Current session context.
    - max_results: Number of results to generate for each query.

    Returns:
    - Dictionary of queries and their respective responses.
    """
    results = {}
    for query in queries:
        try:
            response, session_context = get_recipe_customisation(query, session_context, max_new_tokens=300)
            results[query] = response
        except Exception as e:
            results[query] = f"Error: {str(e)}"
    return results


# Reduced Example Queries
queries = [
    "Suggest three recipes for a gluten-free and low-carb dinner.",
    "How can I make Gazpacho spicier?",
    "Recommend a vegan dessert with chocolate."
]

# Test LLM Responses
session_context = "Context: Recipe recommendations and customisations."
responses = test_llm_responses(queries, session_context)

# Display Results in a Clear Format
for query, response in responses.items():
    print(f"Query: {query}\nResponse:\n{response}\n{'-'*40}")

"""Final Implementation
Features:
 - Batch Processing:

Combines queries into a single request for efficiency.
Saves API costs and reduces processing time.

 - Caching:

Stores results for repeated queries.
Prevents redundant computation.

 - Asynchronous Execution:

Handles multiple queries concurrently.
Improves performance for large batches.

 - Flexible Execution:

Automatically chooses the best method based on query size and performance needs.

Achievements:
Performance Optimisation:

Batch Processing: Processes multiple queries in a single API call, reducing latency and network overhead.
Caching: Uses lru_cache to store previously computed results, which speeds up repeated queries and saves API resources.
Asynchronous Processing: Allows concurrent query handling, enabling quicker responses for multiple queries.
Real-Time Execution:

Using asynchronous execution (asyncio and nest_asyncio), the system can handle multiple queries in real-time without blocking operations.
Flexibility:

Adapts to the size and complexity of the input queries.
Allows toggling between batch processing and async processing based on the use_async parameter.
Scalability:

The solution is designed to handle multiple user queries efficiently, whether they are processed sequentially, in batches, or asynchronously.
Ease of Debugging:

The modular nature of the system (batch_process_queries, get_cached_response, and async_query) makes it easy to isolate and troubleshoot components if issues arise.
"""

import asyncio
from functools import lru_cache
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the Falcon-7B-Instruct model from Hugging Face
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token
print("Model and tokenizer loaded successfully!")

def get_recipe_customisation(user_query, session_context, max_new_tokens=300):
    """
    Generate a recipe suggestion or customisation based on user input.
    """
    # Combine the query with the session context
    input_text = f"Context: {session_context}\n\nUser Query: {user_query}\n\nResponse:"
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def batch_process_queries(queries, session_context, max_new_tokens=300):
    """
    Process multiple queries in a single API call to optimise performance.
    """
    combined_prompt = "\n\n".join([f"Query {i+1}: {query}" for i, query in enumerate(queries)])
    input_text = f"Context: {session_context}\n\nBatch Queries:\n{combined_prompt}\n\nResponses:"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=1024
    )
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True
    )
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    response_lines = response_text.split("\n")
    return {queries[i]: response_lines[i + 1] for i in range(len(queries))}

@lru_cache(maxsize=128)
def get_cached_response(query, session_context, max_new_tokens=300):
    """
    Cached version of the recipe customisation function.
    """
    return get_recipe_customisation(query, session_context, max_new_tokens)

!pip install nest_asyncio

import nest_asyncio
nest_asyncio.apply()

async def async_query(query, session_context, max_new_tokens=300):
    """
    Perform an asynchronous query to the LLM.
    """
    return get_recipe_customisation(query, session_context, max_new_tokens)

async def process_queries_async(queries, session_context, max_new_tokens=300):
    """
    Process multiple queries asynchronously.
    """
    tasks = [async_query(query, session_context, max_new_tokens) for query in queries]
    results = await asyncio.gather(*tasks)
    return {queries[i]: results[i] for i in range(len(queries))}

def combined_test_llm_responses(queries, session_context, use_async=True, max_new_tokens=300):
    """
    Test LLM responses with batch processing, caching, and optional async execution.
    """
    if use_async:
        return asyncio.run(process_queries_async(queries, session_context, max_new_tokens))
    elif len(queries) > 1:
        return batch_process_queries(queries, session_context, max_new_tokens)
    else:
        return {query: get_cached_response(query, session_context, max_new_tokens) for query in queries}

import asyncio

# Queries to test
queries = [
    "Suggest three recipes for a gluten-free and low-carb dinner.",
    "How can I make Gazpacho spicier?",
    "Recommend a vegan dessert with chocolate."
]

# Context for the session
session_context = "Context: Recipe recommendations and customisations."

# Ensure the function handles async internally
responses = combined_test_llm_responses(queries, session_context, use_async=True)

# Display responses
for query, response in responses.items():
    print(f"Query: {query}\nResponse: {response}\n{'-'*40}")





"""Backend API Development and Frontend  or API Demonstration in Postman"""



















"""Performance monitoring: Select metrics that quantify the quality and relevance of the LLM outputs.

Relevance Score: Measure semantic similarity between LLM responses and expected responses using cosine similarity with embeddings.
Execution Time: Monitor the time taken to generate responses.
Response Diversity: Evaluate how varied the responses are for similar queries (e.g., by comparing Jaccard similarity or using n-grams).
Response Completeness: Check if all parts of a response are present (e.g., ingredients, directions, and tips in recipe responses).

Implement Automated Evaluation with Metrics
Create a function that evaluates LLM outputs based on the defined metrics:
"""

import numpy as np
from sklearn.metrics import accuracy_score

def evaluate_llm_responses(queries, responses, expected_outputs, metrics):
    """
    Evaluate LLM responses based on defined metrics.

    Args:
    - queries: List of user queries.
    - responses: List of responses from the LLM.
    - expected_outputs: Expected outputs or ground truth for comparison.
    - metrics: Dictionary defining evaluation criteria.

    Returns:
    - A dictionary with evaluation scores for each metric.
    """
    evaluation_results = {}

    # Relevance: Check if key phrases from the query are in the response
    def relevance_score(query, response):
        return len(set(query.lower().split()) & set(response.lower().split())) / len(query.split())

    # Completeness: Check if the response includes required details
    def completeness_score(response, expected):
        return len(set(expected.lower().split()) & set(response.lower().split())) / len(expected.split())

    # Diversity: Check response uniqueness by comparing responses
    def diversity_score(responses):
        unique_responses = set(responses)
        return len(unique_responses) / len(responses)

    # Accuracy: Compare response with expected output
    def accuracy_score(response, expected):
        return 1 if expected in response else 0

    # Evaluate each metric
    for query, response, expected in zip(queries, responses, expected_outputs):
        evaluation_results[query] = {
            "relevance": relevance_score(query, response),
            "completeness": completeness_score(response, expected),
            "accuracy": accuracy_score(response, expected)
        }

    # Add diversity as a global metric
    evaluation_results["diversity"] = diversity_score(responses)

    return evaluation_results

"""Log and Visualise Performance Over Time
Implement a logging mechanism to track performance trends:
"""

import matplotlib.pyplot as plt

def log_and_visualise_performance(metrics_over_time):
    """
    Visualise performance metrics over time to monitor trends.

    Args:
    - metrics_over_time: A dictionary with timestamps and evaluation results.

    Returns:
    - Performance trend visualisation.
    """
    timestamps = list(metrics_over_time.keys())
    relevance_scores = [metrics["relevance"] for metrics in metrics_over_time.values()]
    completeness_scores = [metrics["completeness"] for metrics in metrics_over_time.values()]
    accuracy_scores = [metrics["accuracy"] for metrics in metrics_over_time.values()]
    diversity_scores = [metrics["diversity"] for metrics in metrics_over_time.values()]

    # Plot performance trends
    plt.figure(figsize=(10, 6))
    plt.plot(timestamps, relevance_scores, label="Relevance")
    plt.plot(timestamps, completeness_scores, label="Completeness")
    plt.plot(timestamps, accuracy_scores, label="Accuracy")
    plt.plot(timestamps, diversity_scores, label="Diversity")
    plt.title("LLM Performance Trends Over Time")
    plt.xlabel("Time")
    plt.ylabel("Score")
    plt.legend()
    plt.grid(True)
    plt.show()

"""Set Alerts for Anomalies
When performance drops below a threshold, trigger alerts to investigate:
"""

def monitor_performance(evaluation_results, thresholds):
    """
    Monitor performance metrics and trigger alerts if thresholds are breached.

    Args:
    - evaluation_results: Dictionary of evaluation scores.
    - thresholds: Dictionary defining acceptable thresholds for each metric.

    Returns:
    - List of alerts for metrics that fall below the threshold.
    """
    alerts = []
    for query, metrics in evaluation_results.items():
        for metric, score in metrics.items():
            if score < thresholds.get(metric, 0.5):  # Default threshold: 0.5
                alerts.append(f"Alert: {metric} for '{query}' below threshold with score {score:.2f}")
    return alerts

# Example Thresholds
thresholds = {
    "relevance": 0.7,
    "completeness": 0.6,
    "accuracy": 0.8,
    "diversity": 0.5
}

# Monitor performance and raise alerts
alerts = monitor_performance(evaluation_results, thresholds)
for alert in alerts:
    print(alert)





"""Simulate API Endpoints in Colab: Simulate API functionality using Python functions."""

!pip install flask

from flask import Flask, request, jsonify
import threading
import requests

# Initialize Flask app
app = Flask(__name__)

# Define an example endpoint
@app.route('/recipe', methods=['POST'])
def get_recipe():
    """
    Simulate an API endpoint that takes user input and returns a recipe suggestion.
    """
    data = request.json  # Get JSON data from the request
    user_query = data.get('query', '')
    response = {
        "query": user_query,
        "recipes": [
            {"title": "Vegan Chocolate Cake", "ingredients": ["cocoa", "flour", "sugar"], "directions": "Mix and bake."},
            {"title": "Gluten-Free Pizza", "ingredients": ["almond flour", "cheese"], "directions": "Mix and bake."},
        ]
    }
    return jsonify(response)

# Run the Flask app in a separate thread
def run_app():
    app.run(host='0.0.0.0', port=5000)

thread = threading.Thread(target=run_app)
thread.daemon = True
thread.start()

# Test the endpoint using the requests library
import time
time.sleep(2)  # Wait for the Flask server to start

# Simulate a POST request to the API
test_data = {"query": "Suggest vegan recipes"}
response = requests.post("http://0.0.0.0:5000/recipe", json=test_data)
print(response.json())



!pip install fastapi uvicorn

from fastapi import FastAPI
from pydantic import BaseModel
import threading
import requests

# Initialize FastAPI app
app = FastAPI()

# Define request and response schemas
class RecipeRequest(BaseModel):
    query: str

class RecipeResponse(BaseModel):
    title: str
    ingredients: list
    directions: str

# Define an example endpoint
@app.post("/recipe", response_model=list[RecipeResponse])
async def get_recipe(request: RecipeRequest):
    """
    Simulate an API endpoint that takes user input and returns recipe suggestions.
    """
    recipes = [
        {"title": "Vegan Brownies", "ingredients": ["cocoa", "flour", "sugar"], "directions": "Mix and bake."},
        {"title": "Vegetable Stir Fry", "ingredients": ["broccoli", "carrot", "soy sauce"], "directions": "Stir fry in a pan."},
    ]
    return recipes

# Run the FastAPI app in a separate thread
import uvicorn
def run_app():
    uvicorn.run(app, host="0.0.0.0", port=8000)

thread = threading.Thread(target=run_app)
thread.daemon = True
thread.start()

# Test the endpoint using the requests library
import time
time.sleep(2)  # Wait for the FastAPI server to start

# Simulate a POST request to the API
test_data = {"query": "Suggest vegan recipes"}
response = requests.post("http://0.0.0.0:8000/recipe", json=test_data)
print(response.json())



def simulate_recipe_api(user_query):
    """
    Simulate an API call by returning recipes based on user query.
    """
    response = {
        "query": user_query,
        "recipes": [
            {"title": "Vegan Pancakes", "ingredients": ["flour", "almond milk", "banana"], "directions": "Mix and cook."},
            {"title": "Gluten-Free Tacos", "ingredients": ["corn tortillas", "beans", "salsa"], "directions": "Assemble and serve."},
        ]
    }
    return response

# Simulate an API request
user_query = "Suggest recipes for breakfast"
api_response = simulate_recipe_api(user_query)
print(api_response)



"""Add Visualisation or Frontend: Use Colab widgets for interactivity."""

!pip install ipywidgets

import ipywidgets as widgets
from IPython.display import display, clear_output
import pandas as pd
import numpy as np

# Sample dataset
recipes = pd.DataFrame({
    "title": ["Vegan Pancakes", "Gluten-Free Tacos", "Vegetable Stir Fry", "Chocolate Brownies", "Quinoa Salad"],
    "ingredients": [
        "flour, almond milk, banana",
        "corn tortillas, beans, salsa",
        "broccoli, carrot, soy sauce",
        "cocoa powder, sugar, flour",
        "quinoa, cucumber, tomato"
    ],
    "directions": [
        "Mix and cook on a griddle.",
        "Assemble and serve.",
        "Stir fry in a pan.",
        "Mix and bake in the oven.",
        "Combine and toss with dressing."
    ],
    "is_baking": [False, False, False, True, False]
})

# Widget for user query
query_widget = widgets.Text(
    value='',
    placeholder='Enter keywords (e.g., vegan, gluten-free)',
    description='Query:',
    layout=widgets.Layout(width='500px')
)

# Widget for non-baking prioritisation
non_baking_widget = widgets.Checkbox(
    value=True,
    description='Prioritize non-baking recipes',
    disabled=False
)

# Button to trigger recommendations
button = widgets.Button(description="Get Recommendations", button_style='success')

# Output widget for displaying results
output = widgets.Output()

def recommend_recipes(query, prioritize_non_baking=True, max_results=3):
    """
    Filter recipes based on the query and prioritisation.
    """
    # Basic keyword filtering
    filtered = recipes[recipes['ingredients'].str.contains(query, case=False, na=False)]

    if prioritize_non_baking:
        non_baking = filtered[~filtered['is_baking']]
        if len(non_baking) >= max_results:
            return non_baking.head(max_results)

    return filtered.head(max_results)

# Function to display recommendations
def on_button_click(b):
    with output:
        clear_output()  # Clear previous output
        user_query = query_widget.value
        prioritize_non_baking = non_baking_widget.value

        # Get recommendations
        recommendations = recommend_recipes(user_query, prioritize_non_baking)

        # Display results
        if recommendations.empty:
            print("No recipes found for your query.")
        else:
            for _, row in recommendations.iterrows():
                print(f"Title: {row['title']}")
                print(f"Ingredients: {row['ingredients']}")
                print(f"Directions: {row['directions']}\n")
                print("-" * 50)

# Link button to the function
button.on_click(on_button_click)

# Display the interface
display(query_widget, non_baking_widget, button, output)

import matplotlib.pyplot as plt

def plot_recipe_types():
    counts = recipes['is_baking'].value_counts()
    labels = ['Non-Baking', 'Baking']
    plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90)
    plt.title("Recipe Types")
    plt.show()

# Add a button to display the chart
chart_button = widgets.Button(description="Show Recipe Chart", button_style='info')

def on_chart_button_click(b):
    with output:
        clear_output()
        plot_recipe_types()

chart_button.on_click(on_chart_button_click)

# Display chart button
display(chart_button)



"""Save and Share Results: Allow users to download results or save the processed dataset."""

# Save the cleaned dataset as a new CSV
df_cleaned.to_csv('cleaned_recipes.csv', index=False)

# Provide a link to download
from google.colab import files
files.download('cleaned_recipes.csv')